{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss, roc_auc_score, precision_recall_curve, matthews_corrcoef, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import Pool, cv\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import logging\n",
    "import sys\n",
    "import umap.plot\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES_FILE = './data/<YOUR_SENTENCES_FILE>.csv'\n",
    "EMBEDDINGS_FILE = './data/<YOUR_EMBEDDINGS_FILE>.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array(['tfidf', 'bertlegal4', 'multilingua4', 'beto4'], dtype=object)\n",
    "def load_dataset(which, verbose=True):\n",
    "    if which == 'beto4':\n",
    "        filename = \"...\"\n",
    "    elif which == 'bertlegal4':\n",
    "        filename = \"...\"\n",
    "    elif which == 'tfidf':\n",
    "        filename = \"...\"\n",
    "    elif which == 'multilingua4':\n",
    "        sentences_df = pd.read_csv(SENTENCES_FILE)\n",
    "        embeddings = np.load(EMBEDDINGS_FILE)\n",
    "        if verbose:\n",
    "            print(which, SENTENCES_FILE, sentences_df.shape)\n",
    "            print(which, EMBEDDINGS_FILE, embeddings.shape)\n",
    "        data_df = pd.DataFrame(embeddings)\n",
    "        data_df['bias'] = sentences_df['bias']\n",
    "        return data_df\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    data_df = pd.read_csv(filename)\n",
    "    data_df = data_df.drop(['doc','text','page'], axis=1)\n",
    "    if verbose:\n",
    "        print(which, filename, data_df.shape)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = load_dataset('beto4')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio between positive and negative examples\n",
    "scale_pos_weight = len(data_df[data_df.bias==0]) / len(data_df[data_df.bias==1]) \n",
    "print(scale_pos_weight)\n",
    "data_df['bias'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(df, test_size=0.2, random_seed=4, plot=False, weight=1, metric='MCC', learning_rate=0.034741, verbose=True, extra_results=False):\n",
    "     if verbose:\n",
    "          print(metric)\n",
    "     X = df.drop('bias', axis=1)\n",
    "     y = df['bias']\n",
    "     \n",
    "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_seed)\n",
    "     if verbose:\n",
    "          model = CatBoostClassifier(loss_function='Logloss', eval_metric=metric, learning_rate=learning_rate,\n",
    "                              scale_pos_weight=weight, random_seed=random_seed, verbose=verbose)\n",
    "     else:\n",
    "          model = CatBoostClassifier(loss_function='Logloss', eval_metric=metric, learning_rate=learning_rate,\n",
    "                              scale_pos_weight=weight, random_seed=random_seed, logging_level='Silent')\n",
    "     # https://forecastegy.com/posts/catboost-binary-classification-python/\n",
    "     model.fit(X_train, y_train, eval_set=(X_test, y_test), \n",
    "               verbose_eval=300,\n",
    "               early_stopping_rounds=500,\n",
    "               use_best_model=True,\n",
    "               plot=plot)\n",
    "     \n",
    "     class_predictions = model.predict(X_test)\n",
    "     probability_predictions = model.predict_proba(X_test)\n",
    "     \n",
    "     if verbose:\n",
    "          log_loss_value = log_loss(y_test, probability_predictions[:,1])\n",
    "          print(f'Log Loss: {log_loss_value}')\n",
    "     \n",
    "          roc_auc = roc_auc_score(y_test, probability_predictions[:,1])\n",
    "          print(f'ROC AUC: {roc_auc}')\n",
    "     \n",
    "          mcc = matthews_corrcoef(y_test, class_predictions)\n",
    "          print(f'MCC: {mcc}')\n",
    "     \n",
    "     class_report = classification_report(y_test, class_predictions)\n",
    "     if verbose:\n",
    "          print(f'Classification Report:\\n {class_report}')\n",
    "     \n",
    "     if plot:\n",
    "          #Â ConfusionMatrixDisplay.from_predictions(y_test, class_predictions)\n",
    "          ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
    "          plt.show()\n",
    "\n",
    "     if extra_results:\n",
    "          return model, X_test, y_test, classification_report(y_test, class_predictions, output_dict=True)\n",
    "     else:\n",
    "          return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosseval_catboost(df, kfold=5, random_seed=4, plot=False, weight=1, metric='MCC', learning_rate=0.034741):\n",
    "    print(metric)\n",
    "    X = df.drop('bias', axis=1)\n",
    "    y = df['bias']\n",
    "\n",
    "    params = {\"iterations\": 500,\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"random_seed\": random_seed,\n",
    "          #\"depth\": 2,\n",
    "          \"loss_function\": \"Logloss\",\n",
    "          \"eval_metric\": metric,\n",
    "          \"scale_pos_weight\": weight, \n",
    "          \"logging_level\": 'Silent'\n",
    "          #\"verbose\": False\n",
    "    }\n",
    "\n",
    "    scores = cv(\n",
    "        params = params,\n",
    "        pool = Pool(data=X,label=y),\n",
    "        fold_count=kfold,\n",
    "        shuffle=True,\n",
    "        #partition_random_seed=0,\n",
    "        plot=plot,\n",
    "        stratified=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_value_logloss = scores['test-Logloss-mean'].min()\n",
    "    best_iter_logloss = scores['test-Logloss-mean'].values.argmin()\n",
    "    std_logloss = scores['test-Logloss-std'].values[best_iter_logloss]\n",
    "    best_value_metric = scores['test-'+metric+'-mean'].max()\n",
    "    best_iter_metric = scores['test-'+metric+'-mean'].values.argmax()\n",
    "    std_metric = scores['test-'+metric+'-std'].values[best_iter_metric]\n",
    "\n",
    "    print(f'Best Log Loss: {best_value_logloss} at iteration {best_iter_logloss}, std: {std_logloss}')\n",
    "    print(f'Best {metric}: {best_value_metric} at iteration {best_iter_metric}, std: {std_metric}')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, df, metric, kfolds=5, iterations=500, depth=2): \n",
    "    params = {\n",
    "        \"iterations\": iterations,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True), \n",
    "        \"random_seed\": trial.suggest_int(\"random_seed\", 1, 10),\n",
    "        \"depth\": depth, \n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": metric, #\"Recall\", # \"MCC\",\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1, 25, step=0.5),\n",
    "        \"logging_level\": 'Silent'\n",
    "    }\n",
    "\n",
    "    X = df.drop('bias', axis=1)\n",
    "    y = df['bias']\n",
    "    scores_df = cv(\n",
    "        params=params,\n",
    "        pool=Pool(data=X,label=y),\n",
    "        fold_count=kfolds,\n",
    "        shuffle=True,\n",
    "        plot=False,\n",
    "        stratified=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_value_logloss = scores_df['test-Logloss-mean'].min()\n",
    "    best_value_metric = scores_df['test-'+metric+'-mean'].max()\n",
    "\n",
    "    rs = params['random_seed']\n",
    "    lr = params['learning_rate']\n",
    "    w = params['scale_pos_weight']\n",
    "    best_model, X_test, y_test, class_report = train_catboost(df, weight=w, plot=False, random_seed=rs, metric=metric, learning_rate=lr, \n",
    "                                                    extra_results=True, verbose=False)\n",
    "\n",
    "    trial.set_user_attr('1-precision', class_report['1']['precision'])\n",
    "    trial.set_user_attr('1-recall', class_report['1']['recall'])\n",
    "    trial.set_user_attr('metric', metric)\n",
    "\n",
    "    return best_value_metric\n",
    "\n",
    "\n",
    "def run_optuna(df, study_id, database_name=\"example-study.db\", n_trials=10, metric='MCC'):\n",
    "    print('Optimizing for', metric)\n",
    "    study_name = database_name.replace(\".db\", str(study_id)) # Unique identifier for each study.\n",
    "    storage_name = \"sqlite:///{}\".format(database_name)\n",
    "    print(\"DB:\", storage_name, study_name)\n",
    "\n",
    "    # Run the study\n",
    "    sampler = TPESampler(seed=1)\n",
    "    study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True, sampler=sampler)\n",
    "    obj = lambda trial: objective(trial, df, metric)\n",
    "    study.optimize(obj, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print('Best hyperparameters:', study.best_params)\n",
    "    print('Best '+metric+':', study.best_value)\n",
    "    return study \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyper-parameter tuning, if necessary\n",
    "# study = run_optuna(data_df, study_id=1, n_trials=8, metric='MCC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trial.best_params, trial.best_value)\n",
    "\n",
    "rs = 6 #trial.best_params['random_seed']\n",
    "w = 25 #trial.best_params['scale_pos_weight']\n",
    "lr = 0.002574 #trial.best_params['learning_rate']\n",
    "\n",
    "best_model, X_test, y_test, class_report = train_catboost(data_df, weight=w, plot=True, random_seed=rs, metric='MCC', learning_rate=lr, extra_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "umap_args2 = {'n_neighbors': 15,\n",
    "             'n_components': 2,\n",
    "             'metric': 'cosine'}\n",
    "\n",
    "embeddings = data_df.drop('bias', axis=1).values\n",
    "print(embeddings.shape)\n",
    "\n",
    "umap_model2D = umap.UMAP(**umap_args2, random_state=42).fit(embeddings)\n",
    "u = umap_model2D.transform(embeddings)\n",
    "# crosseval_catboost(data_df, weight=w, plot=True, random_seed=rs, metric='PRAUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias(u, y, title=\"\"):\n",
    "    c = ['gray' if x == 0 else 'r' for x in y]\n",
    "    plt.scatter(u[:,0], u[:,1], c=c, alpha=0.4, s=12)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = best_model.predict(embeddings) \n",
    "plot_bias(u, y, title='Predicted bias (full dataset)')\n",
    "\n",
    "y = data_df['bias'] \n",
    "plot_bias(u, y, title='Actual bias (full dataset)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "\n",
    "umap.plot.points(umap_model2D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
